# ğŸ§  Fine-Tuning Large Language Models (LLMs) Efficiently with Unsloth + LoRA

> A practical, lightweight pipeline for fine-tuning large language models using [Unsloth](https://unsloth.ai) and [LoRA (Low-Rank Adaptation)](https://arxiv.org/abs/2106.09685).  
> Created and maintained by [Firas Tlili](https://medium.com/@firastlili).

---

## ğŸš€ Project Overview

This repository demonstrates how to **fine-tune state-of-the-art language models** such as **Llama 3**, **Mistral**, or **Phi-3** efficiently â€” even on **modest hardware**.

By combining **Unslothâ€™s GPU-optimized training engine** with **parameter-efficient tuning (LoRA)** and optional **4-bit quantization**, you can adapt large LLMs to your custom datasets **up to 2Ã— faster** and with **70% less memory usage** than traditional fine-tuning.

---

## ğŸ“š Table of Contents
- [Motivation](#-motivation)
- [Key Features](#-key-features)
- [Architecture](#-architecture)
- [Installation](#ï¸-installation)
- [Quick Start](#-quick-start)
- [Configuration](#ï¸-configuration)
- [Training Pipeline](#-training-pipeline)
- [Evaluation & Export](#-evaluation--export)
- [Results](#-results)
- [Tips & Best Practices](#-tips--best-practices)
- [Acknowledgments](#-acknowledgments)
- [License](#-license)

---

## ğŸ’¡ Motivation

Fine-tuning large language models from scratch is expensive â€” requiring high-end GPUs and huge memory.  
This project bridges that gap by using **parameter-efficient fine-tuning (PEFT)** with **LoRA** and **Unsloth**, allowing you to:

- Customize powerful open-weight models for your domain (legal, medical, industrial, etc.)
- Train and deploy efficiently on a **single consumer GPU**
- Preserve base-model generalization while injecting new knowledge

---

## âœ¨ Key Features

| Feature | Description |
|:--|:--|
| ğŸª¶ **Unsloth Integration** | Optimized training backend (fast kernels, mixed precision, memory offloading). |
| ğŸ§© **LoRA (Low-Rank Adaptation)** | Fine-tune only a small set of weights for efficiency. |
| ğŸ§  **Quantization Support** | 4-bit or 8-bit loading for reduced VRAM footprint. |
| ğŸ“Š **Supervised Fine-Tuning (SFT)** | Instruction-response or conversational data. |
| ğŸ§ª **Evaluation & Merging** | Merge LoRA adapters into the base model for deployment. |
| ğŸ–¥ï¸ **Colab/Local Ready** | Compatible with Google Colab or local GPUs. |

---

## ğŸ—ï¸ Architecture
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Dataset â”‚
â”‚ (Instruction, Input, Response pairs) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Base Model Loader (Unsloth) â”‚
â”‚ â†’ Load model in 4-bit / 8-bit precision â”‚
â”‚ â†’ Freeze base weights â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LoRA Adapter Injection â”‚
â”‚ â†’ Add trainable low-rank matrices â”‚
â”‚ â†’ Define rank (r), alpha, dropout â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Fine-Tuning (SFT Trainer) â”‚
â”‚ â†’ Train only adapter weights â”‚
â”‚ â†’ Evaluate on validation split â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Merge / Export Model â”‚
â”‚ â†’ Merge LoRA into base weights â”‚
â”‚ â†’ Save and deploy (HF Hub / local) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
