# ğŸ§  Fine-Tuning Large Language Models (LLMs) Efficiently with Unsloth + LoRA

> A practical, lightweight pipeline for fine-tuning large language models using [Unsloth](https://unsloth.ai) and [LoRA (Low-Rank Adaptation)](https://arxiv.org/abs/2106.09685).  
> Created and maintained by [Firas Tlili](https://medium.com/@firastlili).

---
![thumbnail](thunb.png)

## ğŸš€ Project Overview

This repository demonstrates how to **fine-tune state-of-the-art language models** such as **Llama 3**, **Mistral**, or **Phi-3** efficiently â€” even on **modest hardware**.

By combining **Unslothâ€™s GPU-optimized training engine** with **parameter-efficient tuning (LoRA)** and optional **4-bit quantization**, you can adapt large LLMs to your custom datasets **up to 2Ã— faster** and with **70% less memory usage** than traditional fine-tuning.


## ğŸ’¡ Motivation

Fine-tuning large language models from scratch is expensive â€” requiring high-end GPUs and huge memory.  
This project bridges that gap by using **parameter-efficient fine-tuning (PEFT)** with **LoRA** and **Unsloth**, allowing you to:

- Customize powerful open-weight models for your domain (legal, medical, industrial, etc.)
- Train and deploy efficiently on a **single consumer GPU**
- Preserve base-model generalization while injecting new knowledge

---

## âœ¨ Key Features

| Feature | Description |
|:--|:--|
| ğŸª¶ **Unsloth Integration** | Optimized training backend (fast kernels, mixed precision, memory offloading). |
| ğŸ§© **LoRA (Low-Rank Adaptation)** | Fine-tune only a small set of weights for efficiency. |
| ğŸ§  **Quantization Support** | 4-bit or 8-bit loading for reduced VRAM footprint. |
| ğŸ“Š **Supervised Fine-Tuning (SFT)** | Instruction-response or conversational data. |
| ğŸ§ª **Evaluation & Merging** | Merge LoRA adapters into the base model for deployment. |
| ğŸ–¥ï¸ **Colab/Local Ready** | Compatible with Google Colab or local GPUs. |

---

## ğŸ—ï¸ Architecture
![Architecture](arch.png)

## ğŸ§  Tips & Best Practices

- ğŸ”¹ **LoRA Rank:** Use ranks **8â€“32** for small datasets; increase for complex or large-scale tasks.  
- ğŸ”¹ **Data Quality:** Always verify dataset formatting â€” avoid trailing spaces, inconsistent keys, or newline issues.  
- ğŸ”¹ **Learning Rate:** Start with **2e-4** and fine-tune based on validation loss trends.  
- ğŸ”¹ **Memory Optimization:** Enable **gradient checkpointing** to save GPU memory.  
- ğŸ”¹ **Evaluation:** Assess both **quantitative metrics** (BLEU, ROUGE, accuracy) and **qualitative results** (response quality).  
- ğŸ”¹ **Deployment:** Merge adapters before serving models â€” set `merge_lora_weights=True`.  
- ğŸ”¹ **Backup:** Regularly back up checkpoints to prevent loss during long runs.

## ğŸ™ Acknowledgments

- ğŸª¶ [**Unsloth**](https://unsloth.ai) â€” for the efficient fine-tuning framework.  
- ğŸ¤— [**Hugging Face Transformers**](https://huggingface.co/transformers) â€” model APIs and tokenizer tools.  
- ğŸ”§ [**PEFT**](https://github.com/huggingface/peft) â€” implementation of LoRA and adapter methods.  
- âœï¸ Original guide by [**Firas Tlili**](https://medium.com/@firastlili/fine-tuning-large-language-models-llms-efficiently-with-unsloth-lora-54b6e10fbfcb).
